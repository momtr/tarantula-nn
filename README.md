
<img src="https://github.com/moritzmitterdorfer/TarantulaNN/blob/master/imgs/logo.png">

# TarantulaNN
ðŸ•·ðŸ•¸ Implementation of a Deep Neural Network with flexibles architectures and activation functions

<br>

Layers:

- FullyConnectedLayer

Activation functions:

- ReLU (*, Rectified Linear Unit)
- Sigmoid
- Tanh

Training:

- mini-batch gradient descent
- stochastic gradient descent

Cost function:

- Sum of squares




