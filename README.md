# TarantulaNN
ðŸ•·ðŸ•¸ Implementation of a Deep Neural Network with flexibles architectures and activation functions

<br>

Layers:

- FullyConnectedLayer

Activation functions:

- ReLU
- Sigmoid
- Tanh

Training:

- mini-batch gradient descent
- stochastic gradient descent

Cost function:

- Sum of squares




