
<img src="https://github.com/moritzmitterdorfer/TarantulaNN/blob/master/imgs/logo.png">

# TarantulaNN
ðŸ•·ðŸ•¸ Implementation of a Deep Neural Network with flexibles architectures and activation functions

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

<br>

Layers:

- FullyConnectedLayer

Activation functions:

- ReLU (*, Rectified Linear Unit)
- Sigmoid
- Tanh

Training:

- mini-batch gradient descent
- stochastic gradient descent

Cost function:

- Sum of squares




